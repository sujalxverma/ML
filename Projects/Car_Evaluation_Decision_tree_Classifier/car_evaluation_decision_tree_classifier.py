# -*- coding: utf-8 -*-
"""Car_Evaluation_Decision_tree_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CA4QopDKJ1lk_HP6REAncWRYRX2DJHj8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# to ignore warnings
import warnings
warnings.filterwarnings('ignore')

# from google.colab import files
# uploaded = files.upload()

carEvaluation = pd.read_csv("carEvaluation.csv")
columnsNames = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'Purchased']
carEvaluation.columns = columnsNames
carEvaluation.describe()
# carEvaluation.isnull().sum()   :   to count null values in the dataframe.
targetCol = 'Purchased'
targetDF = carEvaluation[targetCol]
carEvaluation.drop(targetCol, axis=1, inplace=True)
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(carEvaluation,targetDF,test_size=0.3,random_state=42)


# Since our data is completely categorical, so we need to transform it in such a way that our model will work it on, as generally ML works on numnerical data.
# !pip install category_encoders    # category_encoders is not present by default , for only google collab
# Feature Engineering
import category_encoders as ce
encoders = ce.OrdinalEncoder(['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])
X_train = encoders.fit_transform(X_train)
X_test = encoders.transform(X_test)

# Creating our Decision Tree Classifier model
from sklearn.tree import DecisionTreeClassifier
model_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)   # for our first model we are using entropy as the criteria to decide the measure the quality of a split at each node.
model_entropy.fit(X_train, y_train)
# print(model_entropy.score(X_train, y_train))
# print(model_entropy.score(X_test, y_test))

from sklearn.tree import DecisionTreeClassifier
model_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)   # for our first model we are using gini impurity as the criteria to decide the measure the quality of a split at each node.
model_gini.fit(X_train, y_train)
# print(model_gini.score(X_train, y_train))
# print(model_gini.score(X_test, y_test))


# Visualizing the tree structure of our models involving gini impurity as split criteria.
# plt.figure(figsize=(12,8))
# from sklearn import tree
# tree.plot_tree(model_entropy.fit(X_train, y_train))
# # Visualizing the tree structure of our models involving entropy as split criteria.
# import graphviz
# dot_data = tree.export_graphviz(model_entropy, out_file=None,
#                               feature_names=X_train.columns,
#                               class_names=y_train,
#                               filled=True, rounded=True,
#                               special_characters=True)

# graph = graphviz.Source(dot_data)
# plt.show()





import joblib 
joblib.dump(model_gini, 'Car_Decision_Tree_Model_gini.pkl')
joblib.dump(model_entropy, 'Car_Decision_Tree_Model_entropy.pkl')

from sklearn.metrics import classification_report

print("Gini Classifier Report:")
print(classification_report(y_test, model_gini.predict(X_test)))

print("Entropy Classifier Report:")
print(classification_report(y_test, model_entropy.predict(X_test)))

sample = pd.DataFrame([{
    'buying': 'vhigh',
    'maint': 'low',
    'doors': '4',
    'persons': 'more',
    'lug_boot': 'big',
    'safety': 'high'
}])

# Now, we can take different input value and check the predicted value.
sample_encoded = encoders.transform(sample)
prediction = model_gini.predict(sample_encoded)
print("Predicted class:", prediction[0])
